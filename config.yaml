# config.yaml
# This file manages the configuration for the Universal Generative AI Gateway.
# It defines available models, routing strategies, security, and other operational parameters.

# Section 5.1: Configuration Schema
# Defines all the available LLM deployments the gateway can route to.
model_list:
  - model_name: "gpt-4.1-mini" # Logical group name for this model
    litellm_params:
      model: "gpt-4.1-mini"
      api_key: "os.environ/OPENAI_API_KEY"
      end_point: ""
      api_version: ""
    # Per-deployment rate limits can be defined here
    # rpm: 60
    # tpm: 10000

  - model_name: "gpt-4-turbo"
    litellm_params:
      model: "gpt-4-turbo"
      api_key: "os.environ/OPENAI_API_KEY"
      end_point: ""
      api_version: ""

  - model_name: "claude-3-5-sonnet"
    litellm_params:
      model: "claude-3-5-sonnet-20240620"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      end_point: ""
      api_version: ""


# Section 4.3 & 5.1: Router Settings
# Configures the global behavior of the routing, load balancing, and resilience engine.
router_settings:
  # Defines a user-friendly alias that maps to a specific model group.
  # Requests for "enterprise-chat" will be routed to the "gpt-4-turbo" model group.
  model_group_alias:
    enterprise-chat: "gpt-4-turbo"

  # Default number of retries for failed requests.
  num_retries: 3

  # Default request timeout in seconds.
  timeout: 600

  # Number of consecutive failures before a deployment is temporarily removed from rotation.
  allowed_fails: 5

# Section 2.1 & /5.1: General LiteLLM Settings
litellm_settings:
  # Enables verbose logging from the litellm library for debugging.
  set_verbose: true

  # Global list of models to use as a last resort if a specific model group's fallbacks fail.
  # default_fallbacks: ["gpt-4o"]




